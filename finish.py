# openai GPT ëª¨ë¸ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸° 
import openai
from openai.types import ChatModel
from openai.types.chat import ChatCompletion

## streamlit ê´€ë ¨ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
import streamlit as st
from streamlit.runtime.uploaded_file_manager import UploadedFile

import requests  # For making API calls
import pandas as pd  # For data manipulation
import plotly.express as px  # For data visualization
import re
import json

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.documents.base import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain_core.runnables import Runnable
from langchain.schema.output_parser import StrOutputParser
from langchain_community.document_loaders import PyMuPDFLoader
from typing import List
import os
import fitz
import re

## í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
from dotenv import load_dotenv,dotenv_values
load_dotenv()


############################### 1ë‹¨ê³„ : json ë¬¸ì„œë¥¼ ë²¡í„°DBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜ë“¤ ##########################

## 1: JSON ë°ì´í„°ë¥¼ Documentë¡œ ë³€í™˜
def json_to_documents(json_data: dict) -> List[Document]:
    documents = []
    # Assuming the JSON is structured with data you want to convert into documents
    if isinstance(json_data, list):
        for item in json_data:
            if 'content' in item:
                doc = Document(page_content=item['content'], metadata={"source": "API"})
                documents.append(doc)
    else:
        # Handling a single JSON object if it's not an array
        for key, value in json_data.items():
            if isinstance(value, str):
                doc = Document(page_content=value, metadata={"key": key, "source": "API"})
                documents.append(doc)
    return documents

## 2: Documentë¥¼ ë” ì‘ì€ documentë¡œ ë³€í™˜
def chunk_documents(documents: List[Document]) -> List[Document]:
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
    return text_splitter.split_documents(documents)

## 3: Documentë¥¼ ë²¡í„°DBë¡œ ì €ì¥
def save_to_vector_store(documents: List[Document]) -> None:
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vector_store = FAISS.from_documents(documents, embedding=embeddings)
    vector_store.save_local("faiss_index")



############################### 2ë‹¨ê³„ : RAG ê¸°ëŠ¥ êµ¬í˜„ê³¼ ê´€ë ¨ëœ í•¨ìˆ˜ë“¤ ##########################


## ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ RAG ì²˜ë¦¬
@st.cache_data
def process_question(user_question):


    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    ## ë²¡í„° DB í˜¸ì¶œ
    new_db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)

    ## ê´€ë ¨ ë¬¸ì„œ 3ê°œë¥¼ í˜¸ì¶œí•˜ëŠ” Retriever ìƒì„±
    retriever = new_db.as_retriever(search_kwargs={"k": 3})
    ## ì‚¬ìš©ì ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ë¬¸ì„œ 3ê°œ ê²€ìƒ‰ 
    retrieve_docs : List[Document] = retriever.invoke(user_question)

    ## RAG ì²´ì¸ ì„ ì–¸
    chain = get_rag_chain()
    ## ì§ˆë¬¸ê³¼ ë¬¸ë§¥ì„ ë„£ì–´ì„œ ì²´ì¸ ê²°ê³¼ í˜¸ì¶œ
    response = chain.invoke({"question": user_question, "context": retrieve_docs})

    return response, retrieve_docs

def get_rag_chain() -> Runnable:
    template = """
    ë‹¤ìŒì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•´ì„œ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜
    - ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µì„ í•´ì¤˜
    - ê°„ê²°í•˜ê²Œ 5ì¤„ ì´ë‚´ë¡œ í•´ì¤˜
    - ê³§ë°”ë¡œ ì‘ë‹µê²°ê³¼ë¥¼ ë§í•´ì¤˜

    ì»¨í…ìŠ¤íŠ¸ : {context}

    ì§ˆë¬¸: {question}

    ì‘ë‹µ:"""

    custom_rag_prompt = PromptTemplate.from_template(template)
    model = ChatOpenAI(model="gpt-4o-mini")

    return custom_rag_prompt | model | StrOutputParser()

## Use GPT to extract the company name
def extract_company_name(user_question):
    openai.api_key = st.secrets["OPENAI_API_KEY"]  # Replace with your OpenAI API key
    
    response = openai.chat.completions.create(
        model = "gpt-3.5-turbo",
        messages=[
            {"role": "user", "content": f"Extract a valid company ticker in English from this question: '{user_question}' State only the ticker"}
        ],
        max_tokens=50,
        temperature=0
    )
    
    company_name = response.choices[0].message.content.strip()
    return company_name

############################### Alpha Vantage API ê´€ë ¨ í•¨ìˆ˜ ##########################

def get_alpha_vantage_data(symbol: str, api_key: str) -> pd.DataFrame:
    """Fetch daily time series data for a given company symbol from Alpha Vantage API."""
    base_url = "https://www.alphavantage.co/query"
    params = {
        "function": "TIME_SERIES_DAILY",
        "symbol": symbol,
        "apikey": api_key
    }
    response = requests.get(base_url, params=params)
    data = response.json()
    
    # Parse data into DataFrame
    if "Time Series (Daily)" in data:
        time_series = data["Time Series (Daily)"]
        df = pd.DataFrame.from_dict(time_series, orient='index')
        df.index = pd.to_datetime(df.index)
        df.columns = ["Open", "High", "Low", "Close", "Volume"]
        df = df.apply(pd.to_numeric)
        return df
    else:
        st.error("Failed to fetch data or invalid API response.")
        return pd.DataFrame()

def get_company_overview(symbol: str, api_key: str) -> dict:
    """Fetch company overview data for a given company symbol from Alpha Vantage API."""
    base_url = "https://www.alphavantage.co/query"
    params = {
        "function": "OVERVIEW",
        "symbol": symbol,
        "apikey": api_key
    }
    response = requests.get(base_url, params=params)
    data = response.json()

    # Check if the response contains valid data
    if data and "Symbol" in data:
        return data  # Return the raw JSON data as a dictionary
    else:
        st.error("Failed to fetch company overview or invalid API response.")
        return {}

def get_market_news(ticker: str, api_key: str) -> dict:
    """Fetch market news related to a given company ticker and topic from Alpha Vantage API."""
    base_url = "https://www.alphavantage.co/query"
    params = {
        "function": "NEWS_SENTIMENT",
        "tickers": ticker,
        "apikey": api_key
    }
    response = requests.get(base_url, params=params)
    data = response.json()
    return data

def main():
    st.set_page_config("SoftlyAI ì±—ë´‡", layout="wide")
    
    st.header("SoftlyAI ì±—ë´‡")

    user_question = st.text_input("ê¸ˆìœµ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”",
                                    placeholder="ê¸ˆìœµ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”")

    if user_question:
        
        company_ticker = extract_company_name(user_question)  
        
        api_key = st.secrets["ALPHAVANTAGE_API_KEY"]  # Store API key in Streamlit secrets for security
        
        if company_ticker:
            with st.spinner("ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
                df = get_alpha_vantage_data(company_ticker, api_key)
                if not df.empty:
                    st.subheader(f"{company_ticker} ì¼ì¼ ì£¼ê°€ ë°ì´í„°")
                    st.dataframe(df.head())
                    # ì‹œê°í™”
                    fig = px.line(df, x=df.index, y="Close", title=f"{company_ticker} ì£¼ê°€ (ì¢…ê°€)")
                    st.plotly_chart(fig)
        
        company_overview = get_company_overview(company_ticker, api_key)
        news_data = get_market_news(company_ticker, api_key)
        st.text(news_data)
        
        json_doc = json_to_documents(news_data)
        smaller_documents = chunk_documents(json_doc)
        save_to_vector_store(smaller_documents)
        
        response, context = process_question(user_question)
        st.write(response)
        i = 0 
        for document in context:
            with st.expander("ê´€ë ¨ ë¬¸ì„œ"):
                st.write(document.page_content)
                file_path = document.metadata.get('source', '')
                page_number = document.metadata.get('page', 0) + 1
                button_key =f"link_{file_path}_{page_number}_{i}"
                reference_button = st.button(f"ğŸ” {os.path.basename(file_path)} pg.{page_number}", key=button_key)
                if reference_button:
                    st.session_state.page_number = str(page_number)
                i = i + 1


if __name__ == "__main__":
    main()

